# Awesome LRMs Safety [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

<a href="">
  <img src="https://img.shields.io/github/stars/WangCheng0116/Awesome-LRMs-Safety?style=flat-square&logo=github" alt="GitHub stars">
</a>
<a href="">
  <img src="https://img.shields.io/github/last-commit/WangCheng0116/Awesome-LRMs-Safety?style=flat-square&logo=github" alt="GitHub Last commit">
</a>

---

This repository contains a carefully curated collection of papers discussed in our survey: "Safety Challenges in Large Reasoning Models: A Survey". As LRMs become increasingly powerful, understanding their safety implications becomes critical for responsible AI development. We created this resource to support researchers and practitioners working in this emerging field. If you find this repo useful for your work or research, it is really appreciated if you star this repository and cite our paper.

## 📚 What's Inside?
- 🔬 Cutting-edge research on LRMs vulnerabilities
- 🛠️ Novel attack methodologies against reasoning models
- 🛡️ Defense strategies and safety alignment techniques
- 🔄 Regular updates as the field evolves

## ✨ How to Contribute

- ⭐ Star this repository to show support
- 🔀 Create a PR if you notice missing papers
- 📣 Share with the research community

## Table of Content
<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->



<!-- END doctoc generated TOC please keep comment here to allow auto update -->